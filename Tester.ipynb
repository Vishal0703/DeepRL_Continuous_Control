{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from model import QNetwork\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, BUF_SIZE,N):\n",
    "        self.N = N\n",
    "        self.queue = deque(maxlen = BUF_SIZE)\n",
    "        \n",
    "    def add(priority, exp):\n",
    "        self.queue.append((priority, exp))\n",
    "        \n",
    "    def sample(batch_size, rollout_length = self.N):\n",
    "        exp = sorted(self.queue, reverse = True)[:batch_size]\n",
    "        st_init = []\n",
    "        st_final = []\n",
    "        action = []\n",
    "        done = []\n",
    "        reward = []\n",
    "        priority = []\n",
    "        for (pr,x) in exp:\n",
    "            priority.append(pr)\n",
    "            st_init.append(x[0])\n",
    "            st_final.append(x[1])\n",
    "            action.append(x[2])\n",
    "            done.append(x[3])\n",
    "            a = []\n",
    "            for i in range(rollout_length):\n",
    "                a.append(x[4+i])\n",
    "            reward.append(a)\n",
    "            \n",
    "        st_init = torch.from_numpy(np.array(st_init)).float().to(device)\n",
    "        st_final = torch.from_numpy(np.array(st_final)).float().to(device)\n",
    "        action = torch.from_numpy(np.array(action)).float().to(device)\n",
    "        done = torch.from_numpy(np.array(done)).float().to(device)\n",
    "        reward = torch.from_numpy(np.array(reward)).float().to(device)\n",
    "        priority = torch.from_numpy(np.array(priority)).float().to(device)\n",
    "        \n",
    "        return st_init, st_final, action, done, reward, priority\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \n",
    "        self.actor_network = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.actor_target_network = QNetwork(state_size, action_size, seed).to(device)\n",
    "        \n",
    "        self.critic_network = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.critic_target_network = QNetwork(state_size, action_size, seed).to(device)\n",
    "        \n",
    "        self.rollout_length = N\n",
    "        self.gamma = GAMMA\n",
    "        self.replaymen = ReplayBuffer(BUF_SIZE, self.rollout_length)\n",
    "        \n",
    "        self.optimizer_actor = optim.Adam(self.actor_network.parameters(), lr=ALPHA)\n",
    "        self.optimizer_critic = optim.Adam(self.critic_network.parameters(), lr=BETA)\n",
    "        self.tau = TAU\n",
    "        \n",
    "    def learn():\n",
    "        st_init, st_final, action, done, reward, priority = self.replaymem.sample(batch_size)  #sampling from minibatch\n",
    "        \n",
    "        root_priority = torch.sqrt(priority)    # squar rooting priorities, since we are using mse loss and \n",
    "                                                # i am multiplying both the target and predicted values with \n",
    "                                                # priorities beforehand, so ((a-b)^2)/k = (a/p - b/p)^2 \n",
    "                                                # where p = sqrt(k)\n",
    "                    \n",
    "        q_initialst = (self.critic_network(st_init, action))/root_priority  # qvalues of (s(i),a(i))\n",
    "\n",
    "        action_finalst = self.actor_target_network(st_final)                # action_values of final state(s(i+N))\n",
    "        q_finalst = self.critic_target_network(st_final, action_finalst)    # q_value of final (s(i+N), afinal)\n",
    "        q_finalst = q_finalst*(1-done)\n",
    "        \n",
    "        disc = 1\n",
    "        gamma = self.gamma\n",
    "        g = []\n",
    "        for _ in range(self.rollout_length):\n",
    "            g.append(disc)\n",
    "            disc *= gamma\n",
    "            \n",
    "        g = np.array(g)\n",
    "        \n",
    "        disc_reward = g*reward                                           # discounted reward\n",
    "        \n",
    "        y_val = (torch.sum(disc_reward, dim = 1) + q_finalst)/root_priority  # target value\n",
    "        \n",
    "        critic_loss = (F.mse_loss(q_initialst, y_val))/BUF_SIZE   # mse loss between target and predicted value\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "        \n",
    "        \n",
    "        action_init = self.actor_network(st_init)          # action values as predicted by actor(policy) network\n",
    "        actor_loss = -self.critic_network(st_init, action_init)   # q_values of (s(i), action_value) by critic network\n",
    "                                                                  # negative sign because of gradient ascent\n",
    "        \n",
    "        self.optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "        \n",
    "        soft_update(actor_network, actor_target_network, self.tau)\n",
    "        soft_update(critic_network, critic_target_network, self.tau)\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([(2, ('j', 'nana')), (1, ('nb', 'kaka'))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = sorted(q)[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ('nb', 'kaka'))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nb'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3],\n",
       "        [ 4,  5,  6],\n",
       "        [ 7,  8,  9]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.2, 0.3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.array([0.1, 0.2, 0.3])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1000,  0.4000,  0.9000],\n",
       "        [ 0.4000,  1.0000,  1.8000],\n",
       "        [ 0.7000,  1.6000,  2.7000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.sum(d, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4000,  3.2000,  5.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.1429,  0.9375,  0.6000], dtype=torch.float64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2143,  0.3750,  0.5400],\n",
       "        [ 0.8571,  0.9375,  1.0800],\n",
       "        [ 1.5000,  1.5000,  1.6200]], dtype=torch.float64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d/e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
