{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from model import QNetwork\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUF_SIZE = 1e5\n",
    "N = 5\n",
    "TAU = 1e-3\n",
    "batch_size = 256\n",
    "GAMMA = 0.99\n",
    "ALPHA = 1e-4\n",
    "BETA = 1e-4\n",
    "TD_EPSILON = 1e-3\n",
    "NOISE_EPSILON = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, BUF_SIZE,N):\n",
    "        self.N = N\n",
    "        self.queue = deque(maxlen = BUF_SIZE)\n",
    "        \n",
    "    def add(priority, exp, lock):\n",
    "        lock.aquire()\n",
    "        self.queue.append((priority, exp))\n",
    "        lock.release()\n",
    "        \n",
    "    def sample(batch_size, lock, rollout_length = self.N):\n",
    "        \n",
    "        lock.aquire()\n",
    "        w = []\n",
    "        for e in self.queue:\n",
    "            w.append(e[0])\n",
    "            \n",
    "        exp = random.choices(self.queue, weights = w, k = batch_size)\n",
    "        \n",
    "        for e in exp:\n",
    "            self.queue.remove(e)\n",
    "            \n",
    "        lock.release()\n",
    "        \n",
    "        st_init = []\n",
    "        st_final = []\n",
    "        action = []\n",
    "        done = []\n",
    "        reward = []\n",
    "        priority = []\n",
    "        for (pr,x) in exp:\n",
    "            priority.append(pr)\n",
    "            st_init.append(x[0])\n",
    "            st_final.append(x[1])\n",
    "            action.append(x[2])\n",
    "            done.append(x[3])\n",
    "            reward.append(x[4])\n",
    "            \n",
    "        st_init = torch.from_numpy(np.array(st_init)).float().to(device)\n",
    "        st_final = torch.from_numpy(np.array(st_final)).float().to(device)\n",
    "        action = torch.from_numpy(np.array(action)).float().to(device)\n",
    "        done = torch.from_numpy(np.array(done)).float().to(device)\n",
    "        reward = torch.from_numpy(np.array(reward)).float().to(device)\n",
    "        priority = torch.from_numpy(np.array(priority)).float().to(device)\n",
    "        priority = priority/torch.mean(priority)\n",
    "        \n",
    "        return st_init, st_final, action, done, reward, priority\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \n",
    "        self.actor_network = ActorNetwork(state_size, action_size, seed).to(device)\n",
    "        self.actor_target_network = ActorNetwork(state_size, action_size, seed).to(device)\n",
    "        \n",
    "        self.critic_network = CriticNetwork(state_size, action_size, seed).to(device)\n",
    "        self.critic_target_network = CriticNetwork(state_size, action_size, seed).to(device)\n",
    "        \n",
    "        self.rollout_length = N\n",
    "        self.gamma = GAMMA\n",
    "        self.batch_size = batch_size\n",
    "        self.replaymem = ReplayBuffer(BUF_SIZE, self.rollout_length)\n",
    "        \n",
    "        self.optimizer_actor = optim.Adam(self.actor_network.parameters(), lr=ALPHA)\n",
    "        self.optimizer_critic = optim.Adam(self.critic_network.parameters(), lr=BETA)\n",
    "        self.tau = TAU\n",
    "        \n",
    "    def learn(lock):\n",
    "        st_init, st_final, action, done, reward, priority = self.replaymem.sample(self.batch_size, lock)  #sampling from minibatch\n",
    "        \n",
    "        root_priority = torch.sqrt(priority)    # squar rooting priorities, since we are using mse loss and \n",
    "                                                # i am multiplying both the target and predicted values with \n",
    "                                                # priorities beforehand, so ((a-b)^2)/k = (a/p - b/p)^2 \n",
    "                                                # where p = sqrt(k)\n",
    "                    \n",
    "        q_initialst = (self.critic_network(st_init, action))/root_priority  # qvalues of (s(i),a(i))\n",
    "\n",
    "        action_finalst = self.actor_target_network(st_final)                # action_values of final state(s(i+N))\n",
    "        q_finalst = self.critic_target_network(st_final, action_finalst)    # q_value of final (s(i+N), afinal)\n",
    "        q_finalst = q_finalst*(1-done)\n",
    "        \n",
    "        disc = 1\n",
    "        gamma = self.gamma\n",
    "        g = []\n",
    "        for _ in range(self.rollout_length):\n",
    "            g.append(disc)\n",
    "            disc *= gamma\n",
    "            \n",
    "        g = np.array(g)\n",
    "        \n",
    "        disc_reward = g*reward                                           # discounted reward\n",
    "        \n",
    "        y_val = (torch.sum(disc_reward, dim = 1) + q_finalst)/root_priority  # target value\n",
    "        \n",
    "        td_error = torch.abs(y_val - q_initialst)*root_priority + TD_EPSILON\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            self.replaymem.add(td_error[i], (st_init[i], st_final[i], action[i], done[i], reward[i]))\n",
    "        \n",
    "        critic_loss = (F.mse_loss(q_initialst, y_val))/BUF_SIZE   # mse loss between target and predicted value\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(self.critic_network.parameters(), 1)  # gradient clipping\n",
    "        self.optimizer_critic.step()\n",
    "        \n",
    "        \n",
    "        action_init = self.actor_network(st_init)          # action values as predicted by actor(policy) network\n",
    "        actor_loss = -self.critic_network(st_init, action_init)   # q_values of (s(i), action_value) by critic network\n",
    "                                                                  # negative sign because of gradient ascent\n",
    "        \n",
    "        self.optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "        \n",
    "        soft_update(actor_network, actor_target_network, self.tau)\n",
    "        soft_update(critic_network, critic_target_network, self.tau)\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, h1_size = 128, h2_size = 128):\n",
    "        \n",
    "        \n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, h1_size)\n",
    "        self.fc2 = nn.Linear(h1_size, h2_size)\n",
    "        self.fc3 = nn.Linear(h2_size, action_size)\n",
    "        \n",
    "        \n",
    "    def forward(x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class CriticNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, hs1_size = 128, ha1_size = 32, h2_size = 64):\n",
    "        \n",
    "        \n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, hs1_size)\n",
    "        self.fca1 = nn.Linear(action_size, ha1_size)\n",
    "        self.fc2 = nn.Linear(hs1_size + ha1_size, h2_size) \n",
    "        self.fc3 = nn.Linear(h2_size, action_size)\n",
    "        \n",
    "        \n",
    "    def forward(x,a):\n",
    "        x = F.relu(self.fcs1(x))\n",
    "        a = F.relu(self.fca1(a))\n",
    "        y = torch.cat((x,a), dim = 1)\n",
    "        y = F.relu(self.fc2(y))\n",
    "        y = F.relu(self.fc3(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(state_size, action_size, seed):\n",
    "        self.actor_network = ActorNetwork(state_size, action_size, seed).to(device)\n",
    "        self.critic_network = CriticNetwork(state_size, action_size, seed).to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([(1, tensor([ 3.0000,  0.4000,  0.2560])),\n",
       "       (2, tensor([ 4.0000,  0.1000,  0.4785]))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = deque(maxlen = 3)\n",
    "b.append((1,torch.tensor([3, 0.4, 0.256])))\n",
    "b.append((2, torch.tensor([4, 0.1, 0.4785])))\n",
    "b.append((3, torch.tensor([5, 0.36, 0.4785])))\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "\n",
    "w = [1/6,2/6,3/6]\n",
    "e = np.random.choice(np.arange(3), size = 2, replace=False, p = w)\n",
    "\n",
    "b.remove(b[e[0]])\n",
    "b\n",
    "# for i in e:\n",
    "#     b.remove(i)\n",
    "#     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 1,  2,  3]), tensor([ 4,  5,  6]), tensor([ 7,  8,  9])]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = np.sort(e)\n",
    "len(e)\n",
    "j = []\n",
    "j.append(torch.tensor([1,2,3]))\n",
    "j.append(torch.tensor([4,5,6]))\n",
    "j.append(torch.tensor([7,8,9]))\n",
    "j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([(1, 'er'), (2, 'ol'), (3, 'kp')])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = deque(maxlen = 3)\n",
    "q.append((1, 'er'))\n",
    "q.append((2, 'ol'))\n",
    "q.append((3, 'kp'))\n",
    "q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nb'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3],\n",
       "        [ 4,  5,  6],\n",
       "        [ 7,  8,  9]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 30], [20, 40]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "l.append([10, 30])\n",
    "l.append([20,40])\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 10,  30],\n",
       "        [ 20,  40]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(l)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 5, 8])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4000,  3.2000,  5.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.1429,  0.9375,  0.6000], dtype=torch.float64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2143,  0.3750,  0.5400],\n",
       "        [ 0.8571,  0.9375,  1.0800],\n",
       "        [ 1.5000,  1.5000,  1.6200]], dtype=torch.float64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d/e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Condition, Lock, Process, Queue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f1(cv,a,q):\n",
    "    cv.acquire()\n",
    "    print(1)\n",
    "    a.append(4)\n",
    "    q.put(a)\n",
    "    cv.release()\n",
    "\n",
    "        \n",
    "def f2(cv,q):\n",
    "    cv.acquire()\n",
    "    print(2)\n",
    "    a = q.get()\n",
    "    print(a)\n",
    "    cv.release()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def f():   \n",
    "    cv = Condition()\n",
    "    a = [1, 2, 3]\n",
    "    q = Queue()\n",
    "    \n",
    "    p1 = Process(target = f1, args=(cv,a,q,))\n",
    "    p2 = Process(target = f2, args=(cv,q,))\n",
    "\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "\n",
    "    p1.join()\n",
    "    p2.join()\n",
    "    \n",
    "f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Lock() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-54f817dad08d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Lock() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "l = Lock(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-68-3e96e6062ea6>, line 67)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-68-3e96e6062ea6>\"\u001b[0;36m, line \u001b[0;32m67\u001b[0m\n\u001b[0;31m    reward = torch.from_numpy(np.array(reward)).float().to(device)\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, BUF_SIZE,N):\n",
    "        self.N = N\n",
    "        self.queue = deque(maxlen = BUF_SIZE)\n",
    "        \n",
    "    def add(self, priority, exp):\n",
    "        #lock.acquire()\n",
    "        self.queue.append((priority, exp))\n",
    "        #lock.release()\n",
    "    def update(self, indices, td_error):\n",
    "        for i in range(len(indices)):\n",
    "            self.queue[indices[i]][0] = td_error[i]\n",
    "        \n",
    "    def sample(self, batch_size, rollout_length = N):\n",
    "\n",
    "    #lock.acquire()\n",
    "        w = []\n",
    "        for e in self.queue:\n",
    "            w.append(e[0])\n",
    "\n",
    "        w = np.array(w)\n",
    "        w /= np.sum(w)\n",
    "        print(self.queue[0])\n",
    "        print(batch_size)\n",
    "\n",
    "        indices = np.random.choice(np.arange(len(self.queue)), size = batch_size, replace = False, p = w)\n",
    "\n",
    "        exp = []\n",
    "        for i in indices:\n",
    "            exp.append(self.queue[i])\n",
    "\n",
    "    #         print(exp[0])\n",
    "    #         indices = np.sort(indices)[::-1]\n",
    "\n",
    "    #         print(indices[0])\n",
    "    #         for i in indices:\n",
    "    #             self.queue.remove(self.queue[i])\n",
    "\n",
    "\n",
    "    #lock.release()\n",
    "\n",
    "        st_init = []\n",
    "        st_final = []\n",
    "        action = []\n",
    "        done = []\n",
    "        reward = []\n",
    "        priority = []\n",
    "        for (pr,x) in exp:\n",
    "            priority.append(pr)\n",
    "            st_init.append(x[0])\n",
    "            st_final.append(x[1])\n",
    "            action.append(x[2].detach())\n",
    "            done.append(x[3])\n",
    "            reward.append(x[4])\n",
    "\n",
    "    #         print(st_init[0])\n",
    "    #         print(st_final[0])\n",
    "    #         print(action[0])\n",
    "    #         print(done[0])\n",
    "    #         print(reward[0])\n",
    "    #         print(priority[0])\n",
    "\n",
    "        st_init = torch.from_numpy(np.vstack(st_init)).float().to(device)\n",
    "        st_final = torch.from_numpy(np.vstack(st_final)).float().to(device)\n",
    "        action = torch.from_numpy(np.vstack(action)).float().to(device)\n",
    "        dones = torch.from_numpy(np.array(done)).float().to(device)\n",
    "        reward = torch.from_numpy(np.array(reward)).float().to(device)\n",
    "        priority = torch.from_numpy(np.array(priority)).float().to(device)\n",
    "        priority = priority/torch.sum(priority)\n",
    "\n",
    "        return st_init, st_final, action, done, reward, priority, indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queue)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
